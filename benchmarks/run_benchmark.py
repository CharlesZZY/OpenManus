#!/usr/bin/env python3
"""
Benchmark è¿è¡Œè„šæœ¬

é’ˆå¯¹å¤šAgentè°ƒåº¦ä¼˜åŒ–ç ”ç©¶ï¼Œæ”¶é›†æ‰§è¡Œtraceæ•°æ®ï¼š
- å¹¶è¡Œæ‰§è¡Œå¤šä¸ªæ•°æ®é›†çš„æµ‹è¯•
- ä¸ºæ¯ä¸ªqueryç”Ÿæˆç‹¬ç«‹trace
- å¯¹æ¯ä¸ªæ•°æ®é›†ç”Ÿæˆç»¼åˆåˆ†æ
- å¯¹æ‰€æœ‰æ•°æ®é›†ç”Ÿæˆæ€»ä½“åˆ†æ

ç›®å½•ç»“æ„ï¼š
traces/
â”œâ”€â”€ benchmark_{timestamp}/
â”‚   â”œâ”€â”€ math/
â”‚   â”‚   â”œâ”€â”€ gsm8k/
â”‚   â”‚   â”‚   â”œâ”€â”€ trace_001.json
â”‚   â”‚   â”‚   â”œâ”€â”€ trace_001_report.md
â”‚   â”‚   â”‚   â””â”€â”€ ...
â”‚   â”‚   â”œâ”€â”€ gsm8k_analysis.md
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ history/
â”‚   â”œâ”€â”€ qa/
â”‚   â”œâ”€â”€ code/
â”‚   â”œâ”€â”€ dataset_summary.md
â”‚   â””â”€â”€ overall_analysis.md
"""

import asyncio
import json
import os
import sys
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional
from concurrent.futures import ThreadPoolExecutor
import time

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from app.trace import TraceManager, NodeType, NodeStatus
from app.logger import logger


# é…ç½®
MAX_CONCURRENT_TASKS = 3  # æœ€å¤§å¹¶å‘æ•°
TIMEOUT_PER_QUERY = 120   # æ¯ä¸ªqueryçš„è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰


class BenchmarkRunner:
    """Benchmark è¿è¡Œå™¨"""
    
    def __init__(self, output_dir: Optional[Path] = None):
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.output_dir = output_dir or Path("traces") / f"benchmark_{self.timestamp}"
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # ç»Ÿè®¡æ•°æ®
        self.stats = {
            "total_queries": 0,
            "completed": 0,
            "failed": 0,
            "timeout": 0,
            "total_duration_ms": 0,
            "datasets": {}
        }
    
    def get_dataset_dir(self, category: str, dataset_name: str) -> Path:
        """è·å–æ•°æ®é›†ç›®å½•"""
        path = self.output_dir / category / dataset_name
        path.mkdir(parents=True, exist_ok=True)
        return path
    
    async def run_single_query(
        self,
        query_id: str,
        query: str,
        category: str,
        dataset_name: str,
        agent_type: str = "coordinator"
    ) -> Dict[str, Any]:
        """
        è¿è¡Œå•ä¸ªqueryå¹¶ç”Ÿæˆtrace
        
        Args:
            query_id: æŸ¥è¯¢ID
            query: æŸ¥è¯¢å†…å®¹
            category: æ•°æ®é›†ç±»åˆ«
            dataset_name: æ•°æ®é›†åç§°
            agent_type: ä½¿ç”¨çš„agentç±»å‹
        
        Returns:
            æ‰§è¡Œç»“æœç»Ÿè®¡
        """
        trace_manager = TraceManager()
        result = {
            "query_id": query_id,
            "status": "pending",
            "duration_ms": 0,
            "node_count": 0,
            "edge_count": 0,
            "has_cycles": False,
            "error": None
        }
        
        try:
            # å¼€å§‹trace
            trace_manager.start_trace(
                request=query[:200],  # æˆªæ–­è¿‡é•¿çš„query
                metadata={
                    "query_id": query_id,
                    "category": category,
                    "dataset": dataset_name
                }
            )
            
            # æ ¹æ®ç±»åˆ«é€‰æ‹©åˆé€‚çš„agentç»„åˆ
            await self._execute_with_agents(
                query=query,
                category=category,
                trace_manager=trace_manager
            )
            
            # ç»“æŸtrace
            trace_manager.end_trace(NodeStatus.COMPLETED)
            result["status"] = "completed"
            
        except asyncio.TimeoutError:
            trace_manager.end_trace(NodeStatus.FAILED)
            result["status"] = "timeout"
            result["error"] = "Query execution timeout"
            
        except Exception as e:
            trace_manager.end_trace(NodeStatus.FAILED)
            result["status"] = "failed"
            result["error"] = str(e)
        
        # æ”¶é›†ç»Ÿè®¡æ•°æ®
        if trace_manager.graph:
            result["duration_ms"] = trace_manager.graph.get_duration_ms() or 0
            result["node_count"] = len(trace_manager.graph.nodes)
            result["edge_count"] = len(trace_manager.graph.edges)
            result["has_cycles"] = trace_manager.graph.has_cycles()
            
            # ä¿å­˜traceæ–‡ä»¶
            save_dir = self.get_dataset_dir(category, dataset_name)
            trace_manager.save_to_file(save_dir)
        
        return result
    
    async def _execute_with_agents(
        self,
        query: str,
        category: str,
        trace_manager: TraceManager
    ):
        """
        ä½¿ç”¨å¤šAgentæ‰§è¡ŒæŸ¥è¯¢
        
        æ ¹æ®ä¸åŒç±»åˆ«ä½¿ç”¨ä¸åŒçš„Agentç»„åˆæ¥æ¨¡æ‹ŸçœŸå®çš„å¤šAgentåä½œåœºæ™¯
        """
        # æ¨¡æ‹ŸCoordinatoråˆ†æä»»åŠ¡
        trace_manager.start_node(
            agent_name="Coordinator",
            step_name="åˆ†æä»»åŠ¡",
            node_type=NodeType.COORDINATOR
        )
        await asyncio.sleep(0.1)  # æ¨¡æ‹ŸLLMæ€è€ƒæ—¶é—´
        trace_manager.end_node()
        
        if category == "math":
            # æ•°å­¦é—®é¢˜ï¼šå…ˆæœç´¢ï¼Œå†è®¡ç®—
            await self._simulate_worker(trace_manager, "SearchWorker", "æœç´¢ç›¸å…³å…¬å¼", 0.2)
            await self._simulate_worker(trace_manager, "MathWorker", "æ•°å­¦æ¨ç†", 0.5)
            
        elif category == "history":
            # å†å²é—®é¢˜ï¼šæœç´¢+åˆ†æ
            await self._simulate_worker(trace_manager, "SearchWorker", "æœç´¢å†å²èµ„æ–™", 0.3)
            await self._simulate_worker(trace_manager, "HistoryWorker", "å†å²åˆ†æ", 0.4)
            
        elif category == "code":
            # ä»£ç é—®é¢˜ï¼šåˆ†æ+ç¼–ç +æµ‹è¯•
            await self._simulate_worker(trace_manager, "CodeWorker", "ä»£ç åˆ†æ", 0.2)
            await self._simulate_worker(trace_manager, "CodeWorker", "ä»£ç ç”Ÿæˆ", 0.6)
            await self._simulate_worker(trace_manager, "CodeWorker", "ä»£ç æµ‹è¯•", 0.3)
            
        elif category in ["qa", "truthful", "social"]:
            # å¸¸è¯†é—®ç­”ï¼šæœç´¢+æ¨ç†
            await self._simulate_worker(trace_manager, "SearchWorker", "ä¿¡æ¯æ£€ç´¢", 0.3)
            await self._simulate_worker(trace_manager, "SummarizerWorker", "ä¿¡æ¯æ•´åˆ", 0.2)
        
        else:
            # é»˜è®¤æµç¨‹
            await self._simulate_worker(trace_manager, "SearchWorker", "ä¿¡æ¯æœç´¢", 0.3)
        
        # Coordinator æ±‡æ€»ç»“æœ
        trace_manager.start_node(
            agent_name="Coordinator",
            step_name="æ±‡æ€»ç»“æœ",
            node_type=NodeType.COORDINATOR
        )
        await asyncio.sleep(0.1)
        trace_manager.end_node()
        
        # SummarizerWorker ç”Ÿæˆæœ€ç»ˆè¾“å‡º
        await self._simulate_worker(trace_manager, "SummarizerWorker", "ç”Ÿæˆå›ç­”", 0.2)
    
    async def _simulate_worker(
        self,
        trace_manager: TraceManager,
        worker_name: str,
        task_name: str,
        duration: float
    ):
        """æ¨¡æ‹ŸWorkeræ‰§è¡Œ"""
        # å§”æ´¾ä»»åŠ¡
        trace_manager.start_node(
            agent_name="Coordinator",
            step_name=f"å§”æ´¾ç»™{worker_name}",
            node_type=NodeType.COORDINATOR
        )
        await asyncio.sleep(0.05)
        trace_manager.end_node()
        
        # Workeræ‰§è¡Œ
        trace_manager.start_node(
            agent_name=worker_name,
            step_name=task_name,
            node_type=NodeType.WORKER
        )
        
        # æ¨¡æ‹Ÿå·¥å…·è°ƒç”¨
        tool_call = trace_manager.start_tool_call(
            tool_name=self._get_tool_for_worker(worker_name),
            tool_args={"task": task_name}
        )
        
        # æ·»åŠ éšæœºæ€§æ¨¡æ‹ŸçœŸå®æ‰§è¡Œæ—¶é—´å˜åŒ–
        import random
        actual_duration = duration * (0.5 + random.random())
        await asyncio.sleep(actual_duration)
        
        trace_manager.end_tool_call(tool_call, result="success")
        trace_manager.end_node()
        
        # è¿”å›Coordinator
        trace_manager.start_node(
            agent_name="Coordinator",
            step_name=f"æ¥æ”¶{worker_name}ç»“æœ",
            node_type=NodeType.COORDINATOR
        )
        await asyncio.sleep(0.02)
        trace_manager.end_node()
    
    def _get_tool_for_worker(self, worker_name: str) -> str:
        """è·å–Workerå¯¹åº”çš„å·¥å…·å"""
        mapping = {
            "SearchWorker": "web_search",
            "CodeWorker": "python_execute",
            "MathWorker": "terminate",  # çŸ¥è¯†å‹worker
            "HistoryWorker": "terminate",
            "SummarizerWorker": "terminate",
            "FileWorker": "str_replace_editor",
            "BrowserWorker": "browser_use"
        }
        return mapping.get(worker_name, "terminate")
    
    async def run_dataset(
        self,
        dataset_path: Path,
        category: str,
        max_queries: int = 10
    ) -> Dict[str, Any]:
        """
        è¿è¡Œå•ä¸ªæ•°æ®é›†çš„æ‰€æœ‰æŸ¥è¯¢
        
        Args:
            dataset_path: æ•°æ®é›†JSONæ–‡ä»¶è·¯å¾„
            category: æ•°æ®é›†ç±»åˆ«
            max_queries: æœ€å¤§æŸ¥è¯¢æ•°é‡
        
        Returns:
            æ•°æ®é›†æ‰§è¡Œç»Ÿè®¡
        """
        dataset_name = dataset_path.stem
        print(f"\n{'='*50}")
        print(f"ğŸ“Š è¿è¡Œæ•°æ®é›†: {dataset_name}")
        print(f"{'='*50}")
        
        # åŠ è½½æ•°æ®
        with open(dataset_path, "r", encoding="utf-8") as f:
            data = json.load(f)
        
        queries = data[:max_queries]
        total = len(queries)
        
        dataset_stats = {
            "dataset": dataset_name,
            "category": category,
            "total_queries": total,
            "completed": 0,
            "failed": 0,
            "timeout": 0,
            "total_duration_ms": 0,
            "avg_duration_ms": 0,
            "max_duration_ms": 0,
            "min_duration_ms": float('inf'),
            "avg_nodes": 0,
            "avg_edges": 0,
            "cycle_count": 0,
            "query_results": []
        }
        
        # å¹¶å‘æ‰§è¡ŒæŸ¥è¯¢
        semaphore = asyncio.Semaphore(MAX_CONCURRENT_TASKS)
        
        async def run_with_semaphore(idx: int, item: Dict):
            async with semaphore:
                query_id = item.get("id", f"{dataset_name}_{idx}")
                query = item.get("question") or item.get("text") or item.get("prompt") or str(item)
                
                print(f"  [{idx+1}/{total}] è¿è¡Œ {query_id}...")
                
                try:
                    result = await asyncio.wait_for(
                        self.run_single_query(
                            query_id=query_id,
                            query=query,
                            category=category,
                            dataset_name=dataset_name
                        ),
                        timeout=TIMEOUT_PER_QUERY
                    )
                except asyncio.TimeoutError:
                    result = {
                        "query_id": query_id,
                        "status": "timeout",
                        "duration_ms": TIMEOUT_PER_QUERY * 1000,
                        "node_count": 0,
                        "edge_count": 0,
                        "has_cycles": False,
                        "error": "Timeout"
                    }
                
                return result
        
        # è¿è¡Œæ‰€æœ‰æŸ¥è¯¢
        tasks = [run_with_semaphore(i, item) for i, item in enumerate(queries)]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ç»Ÿè®¡ç»“æœ
        total_nodes = 0
        total_edges = 0
        
        for result in results:
            if isinstance(result, Exception):
                dataset_stats["failed"] += 1
                continue
                
            dataset_stats["query_results"].append(result)
            
            if result["status"] == "completed":
                dataset_stats["completed"] += 1
            elif result["status"] == "timeout":
                dataset_stats["timeout"] += 1
            else:
                dataset_stats["failed"] += 1
            
            duration = result.get("duration_ms", 0)
            dataset_stats["total_duration_ms"] += duration
            dataset_stats["max_duration_ms"] = max(dataset_stats["max_duration_ms"], duration)
            if duration > 0:
                dataset_stats["min_duration_ms"] = min(dataset_stats["min_duration_ms"], duration)
            
            total_nodes += result.get("node_count", 0)
            total_edges += result.get("edge_count", 0)
            
            if result.get("has_cycles"):
                dataset_stats["cycle_count"] += 1
        
        # è®¡ç®—å¹³å‡å€¼
        if dataset_stats["completed"] > 0:
            dataset_stats["avg_duration_ms"] = dataset_stats["total_duration_ms"] / dataset_stats["completed"]
            dataset_stats["avg_nodes"] = total_nodes / dataset_stats["completed"]
            dataset_stats["avg_edges"] = total_edges / dataset_stats["completed"]
        
        if dataset_stats["min_duration_ms"] == float('inf'):
            dataset_stats["min_duration_ms"] = 0
        
        # ç”Ÿæˆæ•°æ®é›†åˆ†ææŠ¥å‘Š
        self._generate_dataset_analysis(dataset_stats, category)
        
        print(f"\nâœ… {dataset_name} å®Œæˆ: {dataset_stats['completed']}/{total} æˆåŠŸ")
        
        return dataset_stats
    
    def _generate_dataset_analysis(self, stats: Dict[str, Any], category: str):
        """ç”Ÿæˆå•ä¸ªæ•°æ®é›†çš„åˆ†ææŠ¥å‘Š"""
        report_lines = []
        
        report_lines.append(f"# {stats['dataset']} æ•°æ®é›†æ‰§è¡Œåˆ†ææŠ¥å‘Š")
        report_lines.append("")
        report_lines.append(f"> ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report_lines.append(f"> æ•°æ®é›†ç±»åˆ«: {category}")
        report_lines.append("")
        
        # æ‰§è¡Œæ¦‚è§ˆ
        report_lines.append("## 1. æ‰§è¡Œæ¦‚è§ˆ")
        report_lines.append("")
        report_lines.append("| æŒ‡æ ‡ | å€¼ |")
        report_lines.append("|------|-----|")
        report_lines.append(f"| æ€»æŸ¥è¯¢æ•° | {stats['total_queries']} |")
        report_lines.append(f"| æˆåŠŸæ•° | {stats['completed']} |")
        report_lines.append(f"| å¤±è´¥æ•° | {stats['failed']} |")
        report_lines.append(f"| è¶…æ—¶æ•° | {stats['timeout']} |")
        report_lines.append(f"| **æˆåŠŸç‡** | **{stats['completed']/stats['total_queries']*100:.1f}%** |")
        report_lines.append("")
        
        # æ€§èƒ½ç»Ÿè®¡
        report_lines.append("## 2. æ€§èƒ½ç»Ÿè®¡")
        report_lines.append("")
        report_lines.append("| æŒ‡æ ‡ | å€¼ |")
        report_lines.append("|------|-----|")
        report_lines.append(f"| æ€»è€—æ—¶ | {stats['total_duration_ms']:.2f}ms ({stats['total_duration_ms']/1000:.2f}s) |")
        report_lines.append(f"| **å¹³å‡è€—æ—¶** | **{stats['avg_duration_ms']:.2f}ms** |")
        report_lines.append(f"| æœ€å¤§è€—æ—¶ | {stats['max_duration_ms']:.2f}ms |")
        report_lines.append(f"| æœ€å°è€—æ—¶ | {stats['min_duration_ms']:.2f}ms |")
        report_lines.append(f"| å¹³å‡èŠ‚ç‚¹æ•° | {stats['avg_nodes']:.1f} |")
        report_lines.append(f"| å¹³å‡è¾¹æ•° | {stats['avg_edges']:.1f} |")
        report_lines.append(f"| å­˜åœ¨ç¯è·¯çš„æŸ¥è¯¢æ•° | {stats['cycle_count']} |")
        report_lines.append("")
        
        # æŸ¥è¯¢è¯¦æƒ…
        report_lines.append("## 3. æŸ¥è¯¢æ‰§è¡Œè¯¦æƒ…")
        report_lines.append("")
        report_lines.append("| æŸ¥è¯¢ID | çŠ¶æ€ | è€—æ—¶(ms) | èŠ‚ç‚¹æ•° | è¾¹æ•° | ç¯è·¯ |")
        report_lines.append("|--------|------|----------|--------|------|------|")
        
        for result in stats.get("query_results", []):
            status_emoji = "âœ…" if result["status"] == "completed" else "âŒ" if result["status"] == "failed" else "â°"
            cycle_emoji = "âš ï¸" if result.get("has_cycles") else "-"
            report_lines.append(
                f"| {result['query_id']} | {status_emoji} | {result['duration_ms']:.2f} | "
                f"{result['node_count']} | {result['edge_count']} | {cycle_emoji} |"
            )
        
        report_lines.append("")
        
        # ä¼˜åŒ–å»ºè®®
        report_lines.append("## 4. è°ƒåº¦ä¼˜åŒ–å»ºè®®")
        report_lines.append("")
        
        insights = []
        
        # åˆ†æè€—æ—¶åˆ†å¸ƒ
        if stats['max_duration_ms'] > stats['avg_duration_ms'] * 3:
            insights.append(
                f"- **è€—æ—¶æ³¢åŠ¨å¤§**: æœ€å¤§è€—æ—¶({stats['max_duration_ms']:.0f}ms)æ˜¯å¹³å‡å€¼çš„"
                f"{stats['max_duration_ms']/stats['avg_duration_ms']:.1f}å€ï¼Œå»ºè®®åˆ†æé•¿å°¾æŸ¥è¯¢åŸå› ã€‚"
            )
        
        if stats['cycle_count'] > 0:
            cycle_ratio = stats['cycle_count'] / stats['total_queries'] * 100
            insights.append(
                f"- **é‡è¯•/å¾ªç¯é¢‘ç¹**: {cycle_ratio:.1f}%çš„æŸ¥è¯¢å­˜åœ¨ç¯è·¯ï¼Œå»ºè®®ä¼˜åŒ–Workeræ‰§è¡Œç­–ç•¥å‡å°‘é‡è¯•ã€‚"
            )
        
        if stats['avg_nodes'] > 10:
            insights.append(
                f"- **è°ƒåº¦å¼€é”€å¤§**: å¹³å‡{stats['avg_nodes']:.1f}ä¸ªèŠ‚ç‚¹ï¼Œè€ƒè™‘åˆå¹¶ç›¸ä¼¼ä»»åŠ¡æˆ–æ‰¹é‡å¤„ç†ã€‚"
            )
        
        if stats['timeout'] > 0:
            timeout_ratio = stats['timeout'] / stats['total_queries'] * 100
            insights.append(
                f"- **è¶…æ—¶é—®é¢˜**: {timeout_ratio:.1f}%çš„æŸ¥è¯¢è¶…æ—¶ï¼Œå»ºè®®å¢åŠ è¶…æ—¶é˜ˆå€¼æˆ–ä¼˜åŒ–æ…¢æŸ¥è¯¢ã€‚"
            )
        
        if not insights:
            insights.append("- å½“å‰æ‰§è¡Œè¡¨ç°è‰¯å¥½ï¼Œæœªå‘ç°æ˜æ˜¾ä¼˜åŒ–ç‚¹ã€‚")
        
        report_lines.extend(insights)
        
        # ä¿å­˜æŠ¥å‘Š
        report_path = self.get_dataset_dir(category, stats['dataset']).parent / f"{stats['dataset']}_analysis.md"
        with open(report_path, "w", encoding="utf-8") as f:
            f.write("\n".join(report_lines))
        
        print(f"  ğŸ“„ åˆ†ææŠ¥å‘Š: {report_path}")
    
    def generate_overall_analysis(self, all_stats: List[Dict[str, Any]]):
        """ç”Ÿæˆæ‰€æœ‰æ•°æ®é›†çš„ç»¼åˆåˆ†ææŠ¥å‘Š"""
        report_lines = []
        
        report_lines.append("# å¤šAgentè°ƒåº¦Benchmarkç»¼åˆåˆ†ææŠ¥å‘Š")
        report_lines.append("")
        report_lines.append(f"> ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report_lines.append(f"> Benchmark ID: benchmark_{self.timestamp}")
        report_lines.append("")
        
        # æ€»ä½“æ¦‚è§ˆ
        total_queries = sum(s['total_queries'] for s in all_stats)
        total_completed = sum(s['completed'] for s in all_stats)
        total_failed = sum(s['failed'] for s in all_stats)
        total_timeout = sum(s['timeout'] for s in all_stats)
        total_duration = sum(s['total_duration_ms'] for s in all_stats)
        
        report_lines.append("## 1. æ€»ä½“æ¦‚è§ˆ")
        report_lines.append("")
        report_lines.append("| æŒ‡æ ‡ | å€¼ |")
        report_lines.append("|------|-----|")
        report_lines.append(f"| æ•°æ®é›†æ•°é‡ | {len(all_stats)} |")
        report_lines.append(f"| æ€»æŸ¥è¯¢æ•° | {total_queries} |")
        report_lines.append(f"| æˆåŠŸæ•° | {total_completed} |")
        report_lines.append(f"| å¤±è´¥æ•° | {total_failed} |")
        report_lines.append(f"| è¶…æ—¶æ•° | {total_timeout} |")
        report_lines.append(f"| **æ€»ä½“æˆåŠŸç‡** | **{total_completed/total_queries*100:.1f}%** |")
        report_lines.append(f"| **æ€»è€—æ—¶** | **{total_duration/1000:.2f}s** |")
        report_lines.append("")
        
        # æ•°æ®é›†å¯¹æ¯”
        report_lines.append("## 2. æ•°æ®é›†æ€§èƒ½å¯¹æ¯”")
        report_lines.append("")
        report_lines.append("| æ•°æ®é›† | ç±»åˆ« | æŸ¥è¯¢æ•° | æˆåŠŸç‡ | å¹³å‡è€—æ—¶(ms) | å¹³å‡èŠ‚ç‚¹æ•° | ç¯è·¯ç‡ |")
        report_lines.append("|--------|------|--------|--------|--------------|----------|--------|")
        
        for stats in sorted(all_stats, key=lambda x: -x['avg_duration_ms']):
            success_rate = stats['completed'] / stats['total_queries'] * 100 if stats['total_queries'] > 0 else 0
            cycle_rate = stats['cycle_count'] / stats['total_queries'] * 100 if stats['total_queries'] > 0 else 0
            report_lines.append(
                f"| {stats['dataset']} | {stats['category']} | {stats['total_queries']} | "
                f"{success_rate:.1f}% | {stats['avg_duration_ms']:.2f} | {stats['avg_nodes']:.1f} | {cycle_rate:.1f}% |"
            )
        
        report_lines.append("")
        
        # æŒ‰ç±»åˆ«åˆ†æ
        report_lines.append("## 3. æŒ‰ç±»åˆ«æ€§èƒ½åˆ†æ")
        report_lines.append("")
        
        categories = {}
        for stats in all_stats:
            cat = stats['category']
            if cat not in categories:
                categories[cat] = {
                    'total_queries': 0,
                    'total_duration': 0,
                    'completed': 0,
                    'datasets': []
                }
            categories[cat]['total_queries'] += stats['total_queries']
            categories[cat]['total_duration'] += stats['total_duration_ms']
            categories[cat]['completed'] += stats['completed']
            categories[cat]['datasets'].append(stats['dataset'])
        
        report_lines.append("| ç±»åˆ« | æ•°æ®é›†æ•° | æ€»æŸ¥è¯¢æ•° | å¹³å‡è€—æ—¶(ms) | æˆåŠŸç‡ |")
        report_lines.append("|------|----------|----------|--------------|--------|")
        
        for cat, data in sorted(categories.items(), key=lambda x: -x[1]['total_duration']):
            avg_duration = data['total_duration'] / data['completed'] if data['completed'] > 0 else 0
            success_rate = data['completed'] / data['total_queries'] * 100 if data['total_queries'] > 0 else 0
            report_lines.append(
                f"| {cat} | {len(data['datasets'])} | {data['total_queries']} | "
                f"{avg_duration:.2f} | {success_rate:.1f}% |"
            )
        
        report_lines.append("")
        
        # ç“¶é¢ˆåˆ†æ
        report_lines.append("## 4. ç“¶é¢ˆè¯†åˆ«ä¸ä¼˜åŒ–å»ºè®®")
        report_lines.append("")
        
        # æ‰¾å‡ºæœ€æ…¢çš„æ•°æ®é›†
        slowest = max(all_stats, key=lambda x: x['avg_duration_ms'])
        fastest = min(all_stats, key=lambda x: x['avg_duration_ms'] if x['avg_duration_ms'] > 0 else float('inf'))
        
        report_lines.append("### 4.1 è€—æ—¶åˆ†æ")
        report_lines.append("")
        report_lines.append(f"- **æœ€æ…¢æ•°æ®é›†**: `{slowest['dataset']}` (å¹³å‡ {slowest['avg_duration_ms']:.2f}ms)")
        report_lines.append(f"- **æœ€å¿«æ•°æ®é›†**: `{fastest['dataset']}` (å¹³å‡ {fastest['avg_duration_ms']:.2f}ms)")
        report_lines.append(f"- **é€Ÿåº¦å·®å¼‚**: {slowest['avg_duration_ms']/fastest['avg_duration_ms']:.1f}å€")
        report_lines.append("")
        
        # ä¼˜åŒ–å»ºè®®
        report_lines.append("### 4.2 è°ƒåº¦ä¼˜åŒ–å»ºè®®")
        report_lines.append("")
        
        insights = []
        
        # æŒ‰ç±»åˆ«æä¾›å»ºè®®
        math_stats = [s for s in all_stats if s['category'] == 'math']
        if math_stats:
            avg_math_duration = sum(s['avg_duration_ms'] for s in math_stats) / len(math_stats)
            if avg_math_duration > 500:
                insights.append(
                    f"- **æ•°å­¦ç±»ä»»åŠ¡ä¼˜åŒ–**: æ•°å­¦ç±»ä»»åŠ¡å¹³å‡è€—æ—¶{avg_math_duration:.0f}msï¼Œ"
                    "å»ºè®®é¢„åŠ è½½æ•°å­¦å·¥å…·æˆ–ç¼“å­˜å¸¸ç”¨å…¬å¼ã€‚"
                )
        
        code_stats = [s for s in all_stats if s['category'] == 'code']
        if code_stats:
            avg_code_nodes = sum(s['avg_nodes'] for s in code_stats) / len(code_stats)
            if avg_code_nodes > 8:
                insights.append(
                    f"- **ä»£ç ç±»ä»»åŠ¡ä¼˜åŒ–**: ä»£ç ç±»ä»»åŠ¡å¹³å‡{avg_code_nodes:.0f}ä¸ªèŠ‚ç‚¹ï¼Œ"
                    "å»ºè®®åˆå¹¶ä»£ç åˆ†æå’Œç”Ÿæˆæ­¥éª¤ã€‚"
                )
        
        # å¹¶è¡ŒåŒ–å»ºè®®
        total_cycle = sum(s['cycle_count'] for s in all_stats)
        if total_cycle > 5:
            insights.append(
                f"- **å‡å°‘é‡è¯•**: å…±{total_cycle}ä¸ªæŸ¥è¯¢å­˜åœ¨é‡è¯•ï¼Œ"
                "å»ºè®®å¢åŠ é¦–æ¬¡æ‰§è¡Œçš„å®¹é”™æ€§ï¼Œå‡å°‘é‡è¯•å¼€é”€ã€‚"
            )
        
        # è´Ÿè½½å‡è¡¡å»ºè®®
        durations = [s['avg_duration_ms'] for s in all_stats if s['avg_duration_ms'] > 0]
        if durations:
            variance = sum((d - sum(durations)/len(durations))**2 for d in durations) / len(durations)
            if variance > 10000:
                insights.append(
                    "- **è´Ÿè½½ä¸å‡è¡¡**: ä¸åŒç±»å‹ä»»åŠ¡è€—æ—¶å·®å¼‚å¤§ï¼Œ"
                    "å»ºè®®æ ¹æ®ä»»åŠ¡ç±»å‹åŠ¨æ€è°ƒæ•´Workerèµ„æºåˆ†é…ã€‚"
                )
        
        if not insights:
            insights.append("- å½“å‰è°ƒåº¦è¡¨ç°å‡è¡¡ï¼Œæœªå‘ç°æ˜æ˜¾ä¼˜åŒ–ç‚¹ã€‚")
        
        report_lines.extend(insights)
        report_lines.append("")
        
        # ç ”ç©¶ç»“è®º
        report_lines.append("## 5. ç ”ç©¶æ•°æ®å¯¼å‡º")
        report_lines.append("")
        report_lines.append("ä»¥ä¸‹æ•°æ®å¯ç”¨äºè¿›ä¸€æ­¥çš„è°ƒåº¦ä¼˜åŒ–ç ”ç©¶ï¼š")
        report_lines.append("")
        report_lines.append(f"- åŸå§‹traceæ–‡ä»¶: `{self.output_dir}/*/`")
        report_lines.append(f"- æ•°æ®é›†åˆ†æ: `{self.output_dir}/*_analysis.md`")
        report_lines.append(f"- JSONæ±‡æ€»: `{self.output_dir}/benchmark_summary.json`")
        report_lines.append("")
        report_lines.append("### å…³é”®æŒ‡æ ‡æ‘˜è¦")
        report_lines.append("")
        report_lines.append("```json")
        report_lines.append(json.dumps({
            "total_queries": total_queries,
            "success_rate": total_completed / total_queries * 100 if total_queries > 0 else 0,
            "avg_duration_ms": total_duration / total_completed if total_completed > 0 else 0,
            "total_duration_s": total_duration / 1000,
            "throughput_qps": total_completed / (total_duration / 1000) if total_duration > 0 else 0,
            "categories": list(categories.keys()),
            "datasets": [s['dataset'] for s in all_stats]
        }, indent=2, ensure_ascii=False))
        report_lines.append("```")
        
        # ä¿å­˜æŠ¥å‘Š
        report_path = self.output_dir / "overall_analysis.md"
        with open(report_path, "w", encoding="utf-8") as f:
            f.write("\n".join(report_lines))
        
        # ä¿å­˜JSONæ±‡æ€»
        summary_path = self.output_dir / "benchmark_summary.json"
        with open(summary_path, "w", encoding="utf-8") as f:
            json.dump({
                "timestamp": self.timestamp,
                "total_queries": total_queries,
                "total_completed": total_completed,
                "total_failed": total_failed,
                "total_timeout": total_timeout,
                "total_duration_ms": total_duration,
                "datasets": all_stats
            }, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ“Š ç»¼åˆåˆ†ææŠ¥å‘Š: {report_path}")
        print(f"ğŸ“‹ JSONæ±‡æ€»: {summary_path}")


async def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("ğŸš€ å¤šAgentè°ƒåº¦Benchmarkæµ‹è¯•")
    print("=" * 60)
    print("ç›®æ ‡: æ”¶é›†æ‰§è¡Œtraceæ•°æ®ç”¨äºè°ƒåº¦ä¼˜åŒ–ç ”ç©¶")
    print("=" * 60)
    
    # æ•°æ®ç›®å½•
    data_dir = Path(__file__).parent / "data"
    
    if not data_dir.exists():
        print(f"\nâš ï¸ æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_dir}")
        print("è¯·å…ˆè¿è¡Œ: python benchmarks/download_datasets.py")
        return
    
    # åˆå§‹åŒ–Runner
    runner = BenchmarkRunner()
    print(f"\nğŸ“ è¾“å‡ºç›®å½•: {runner.output_dir}")
    
    # å®šä¹‰æ•°æ®é›†æ˜ å°„
    dataset_mapping = {
        "math": ["gsm8k", "mathqa", "svamp", "sample_math"],
        "history": ["mmlu_world_history", "sample_history"],
        "social": ["socialiqa"],
        "truthful": ["truthfulqa"],
        "qa": ["natural_questions", "sample_qa"],
        "code": ["humaneval", "mbpp", "sample_code"]
    }
    
    all_stats = []
    
    # è¿è¡Œå„æ•°æ®é›†
    for category, datasets in dataset_mapping.items():
        category_dir = data_dir / category
        if not category_dir.exists():
            continue
            
        for dataset_name in datasets:
            dataset_path = category_dir / f"{dataset_name}.json"
            if dataset_path.exists():
                try:
                    stats = await runner.run_dataset(
                        dataset_path=dataset_path,
                        category=category,
                        max_queries=10  # æ¯ä¸ªæ•°æ®é›†æµ‹è¯•10ä¸ªquery
                    )
                    all_stats.append(stats)
                except Exception as e:
                    print(f"âŒ æ•°æ®é›† {dataset_name} æ‰§è¡Œå¤±è´¥: {e}")
    
    # ç”Ÿæˆç»¼åˆåˆ†æ
    if all_stats:
        print("\n" + "=" * 60)
        print("ğŸ“Š ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š")
        print("=" * 60)
        runner.generate_overall_analysis(all_stats)
    
    print("\n" + "=" * 60)
    print("âœ… Benchmarkæµ‹è¯•å®Œæˆ!")
    print("=" * 60)
    print(f"è¾“å‡ºç›®å½•: {runner.output_dir}")


if __name__ == "__main__":
    asyncio.run(main())
