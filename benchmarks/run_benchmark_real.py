#!/usr/bin/env python3
"""
ä½¿ç”¨çœŸå®Multi-Agentç³»ç»Ÿè¿è¡ŒBenchmark

ä¸ run_benchmark.py çš„åŒºåˆ«ï¼š
- ä½¿ç”¨çœŸå®çš„ MultiAgentFlow æ‰§è¡ŒæŸ¥è¯¢
- æ”¶é›†çœŸå®çš„LLMè°ƒç”¨å’Œå·¥å…·æ‰§è¡Œtrace
- ç”¨äºç ”ç©¶çœŸå®åœºæ™¯ä¸‹çš„å¤šAgentè°ƒåº¦æ€§èƒ½

æ³¨æ„ï¼šè¿è¡Œæ­¤è„šæœ¬ä¼šæ¶ˆè€—APIé¢åº¦ï¼Œå»ºè®®å…ˆç”¨æ¨¡æ‹Ÿç‰ˆæœ¬æµ‹è¯•
"""

import asyncio
import json
import sys
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from app.agent.coordinator import Coordinator
from app.agent.workers import (
    SearchWorker, CodeWorker, MathWorker, 
    CopywriterWorker, HistoryWorker, SummarizerWorker
)
from app.flow.multi_agent import MultiAgentFlow
from app.trace import TraceManager, NodeType, NodeStatus
from app.logger import logger


# é…ç½®
MAX_CONCURRENT_TASKS = 2  # çœŸå®æ‰§è¡Œæ—¶å¹¶å‘æ•°åº”è¾ƒä½
TIMEOUT_PER_QUERY = 300   # çœŸå®æ‰§è¡Œè¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
MAX_QUERIES_PER_DATASET = 5  # æ¯ä¸ªæ•°æ®é›†æœ€å¤šè¿è¡Œçš„æŸ¥è¯¢æ•°


class RealBenchmarkRunner:
    """ä½¿ç”¨çœŸå®Agentçš„Benchmarkè¿è¡Œå™¨"""
    
    def __init__(self, output_dir: Optional[Path] = None):
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.output_dir = output_dir or Path("traces") / f"real_benchmark_{self.timestamp}"
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        self.stats = {
            "total_queries": 0,
            "completed": 0,
            "failed": 0,
            "timeout": 0,
            "total_duration_ms": 0,
            "datasets": {}
        }
    
    def _create_flow_for_category(self, category: str) -> MultiAgentFlow:
        """æ ¹æ®æ•°æ®é›†ç±»åˆ«åˆ›å»ºé€‚åˆçš„MultiAgentFlow"""
        
        # åˆ›å»ºCoordinator
        coordinator = Coordinator()
        
        # æ ¹æ®ç±»åˆ«é€‰æ‹©Workerç»„åˆ
        workers = {}
        
        if category == "math":
            workers = {
                "search": SearchWorker(),
                "math": MathWorker(),
                "summarizer": SummarizerWorker()
            }
        elif category == "history":
            workers = {
                "search": SearchWorker(),
                "history": HistoryWorker(),
                "summarizer": SummarizerWorker()
            }
        elif category == "code":
            workers = {
                "code": CodeWorker(),
                "summarizer": SummarizerWorker()
            }
        else:  # qa, truthful, social
            workers = {
                "search": SearchWorker(),
                "copywriter": CopywriterWorker(),
                "summarizer": SummarizerWorker()
            }
        
        # åˆ›å»ºå¸¦traceçš„flow
        flow = MultiAgentFlow(
            coordinator=coordinator,
            workers=workers,
            enable_trace=True
        )
        
        return flow
    
    async def run_single_query(
        self,
        query_id: str,
        query: str,
        category: str,
        dataset_name: str
    ) -> Dict[str, Any]:
        """è¿è¡Œå•ä¸ªæŸ¥è¯¢"""
        result = {
            "query_id": query_id,
            "status": "pending",
            "duration_ms": 0,
            "node_count": 0,
            "edge_count": 0,
            "has_cycles": False,
            "error": None
        }
        
        try:
            # åˆ›å»ºFlow
            flow = self._create_flow_for_category(category)
            
            # æ‰§è¡ŒæŸ¥è¯¢
            start_time = datetime.now()
            await asyncio.wait_for(
                flow.execute(query),
                timeout=TIMEOUT_PER_QUERY
            )
            end_time = datetime.now()
            
            result["status"] = "completed"
            result["duration_ms"] = (end_time - start_time).total_seconds() * 1000
            
            # å¦‚æœflowæœ‰traceï¼Œä¿å­˜å®ƒ
            if flow.trace_manager and flow.trace_manager.graph:
                graph = flow.trace_manager.graph
                result["node_count"] = len(graph.nodes)
                result["edge_count"] = len(graph.edges)
                result["has_cycles"] = graph.has_cycles()
                
                # ä¿å­˜trace
                save_dir = self.output_dir / category / dataset_name
                save_dir.mkdir(parents=True, exist_ok=True)
                flow.trace_manager.save_to_file(save_dir)
            
        except asyncio.TimeoutError:
            result["status"] = "timeout"
            result["error"] = f"Timeout after {TIMEOUT_PER_QUERY}s"
            result["duration_ms"] = TIMEOUT_PER_QUERY * 1000
            
        except Exception as e:
            result["status"] = "failed"
            result["error"] = str(e)
            logger.error(f"Query {query_id} failed: {e}")
        
        return result
    
    async def run_dataset(
        self,
        dataset_path: Path,
        category: str,
        max_queries: int = MAX_QUERIES_PER_DATASET
    ) -> Dict[str, Any]:
        """è¿è¡Œå•ä¸ªæ•°æ®é›†"""
        dataset_name = dataset_path.stem
        print(f"\n{'='*50}")
        print(f"ğŸ“Š è¿è¡Œæ•°æ®é›†: {dataset_name} (çœŸå®æ‰§è¡Œ)")
        print(f"{'='*50}")
        
        # åŠ è½½æ•°æ®
        with open(dataset_path, "r", encoding="utf-8") as f:
            data = json.load(f)
        
        queries = data[:max_queries]
        total = len(queries)
        
        dataset_stats = {
            "dataset": dataset_name,
            "category": category,
            "total_queries": total,
            "completed": 0,
            "failed": 0,
            "timeout": 0,
            "total_duration_ms": 0,
            "avg_duration_ms": 0,
            "max_duration_ms": 0,
            "min_duration_ms": float('inf'),
            "avg_nodes": 0,
            "avg_edges": 0,
            "cycle_count": 0,
            "query_results": []
        }
        
        # ä¸²è¡Œæ‰§è¡Œï¼ˆçœŸå®æ‰§è¡Œæ—¶é¿å…è¿‡å¤šå¹¶å‘ï¼‰
        for idx, item in enumerate(queries):
            query_id = item.get("id", f"{dataset_name}_{idx}")
            query = item.get("question") or item.get("text") or item.get("prompt") or str(item)
            
            print(f"  [{idx+1}/{total}] è¿è¡Œ {query_id}...")
            
            result = await self.run_single_query(
                query_id=query_id,
                query=query,
                category=category,
                dataset_name=dataset_name
            )
            
            dataset_stats["query_results"].append(result)
            
            # æ›´æ–°ç»Ÿè®¡
            if result["status"] == "completed":
                dataset_stats["completed"] += 1
                duration = result["duration_ms"]
                dataset_stats["total_duration_ms"] += duration
                dataset_stats["max_duration_ms"] = max(dataset_stats["max_duration_ms"], duration)
                if duration > 0:
                    dataset_stats["min_duration_ms"] = min(dataset_stats["min_duration_ms"], duration)
            elif result["status"] == "timeout":
                dataset_stats["timeout"] += 1
            else:
                dataset_stats["failed"] += 1
            
            if result.get("has_cycles"):
                dataset_stats["cycle_count"] += 1
            
            print(f"      çŠ¶æ€: {result['status']}, è€—æ—¶: {result['duration_ms']:.0f}ms")
        
        # è®¡ç®—å¹³å‡å€¼
        if dataset_stats["completed"] > 0:
            dataset_stats["avg_duration_ms"] = dataset_stats["total_duration_ms"] / dataset_stats["completed"]
            total_nodes = sum(r.get("node_count", 0) for r in dataset_stats["query_results"])
            total_edges = sum(r.get("edge_count", 0) for r in dataset_stats["query_results"])
            dataset_stats["avg_nodes"] = total_nodes / dataset_stats["completed"]
            dataset_stats["avg_edges"] = total_edges / dataset_stats["completed"]
        
        if dataset_stats["min_duration_ms"] == float('inf'):
            dataset_stats["min_duration_ms"] = 0
        
        # ç”Ÿæˆåˆ†ææŠ¥å‘Š
        self._generate_dataset_analysis(dataset_stats, category)
        
        print(f"\nâœ… {dataset_name} å®Œæˆ: {dataset_stats['completed']}/{total} æˆåŠŸ")
        return dataset_stats
    
    def _generate_dataset_analysis(self, stats: Dict[str, Any], category: str):
        """ç”Ÿæˆæ•°æ®é›†åˆ†ææŠ¥å‘Š"""
        report_lines = []
        
        report_lines.append(f"# {stats['dataset']} çœŸå®æ‰§è¡Œåˆ†ææŠ¥å‘Š")
        report_lines.append("")
        report_lines.append(f"> ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report_lines.append(f"> æ‰§è¡Œæ¨¡å¼: çœŸå®Multi-Agentæ‰§è¡Œ")
        report_lines.append(f"> æ•°æ®é›†ç±»åˆ«: {category}")
        report_lines.append("")
        
        # æ‰§è¡Œæ¦‚è§ˆ
        report_lines.append("## æ‰§è¡Œæ¦‚è§ˆ")
        report_lines.append("")
        report_lines.append("| æŒ‡æ ‡ | å€¼ |")
        report_lines.append("|------|-----|")
        report_lines.append(f"| æ€»æŸ¥è¯¢æ•° | {stats['total_queries']} |")
        report_lines.append(f"| æˆåŠŸæ•° | {stats['completed']} |")
        report_lines.append(f"| å¤±è´¥æ•° | {stats['failed']} |")
        report_lines.append(f"| è¶…æ—¶æ•° | {stats['timeout']} |")
        success_rate = stats['completed']/stats['total_queries']*100 if stats['total_queries'] > 0 else 0
        report_lines.append(f"| **æˆåŠŸç‡** | **{success_rate:.1f}%** |")
        report_lines.append("")
        
        # æ€§èƒ½ç»Ÿè®¡
        report_lines.append("## æ€§èƒ½ç»Ÿè®¡")
        report_lines.append("")
        report_lines.append("| æŒ‡æ ‡ | å€¼ |")
        report_lines.append("|------|-----|")
        report_lines.append(f"| æ€»è€—æ—¶ | {stats['total_duration_ms']/1000:.2f}s |")
        report_lines.append(f"| **å¹³å‡è€—æ—¶** | **{stats['avg_duration_ms']:.2f}ms** |")
        report_lines.append(f"| æœ€å¤§è€—æ—¶ | {stats['max_duration_ms']:.2f}ms |")
        report_lines.append(f"| æœ€å°è€—æ—¶ | {stats['min_duration_ms']:.2f}ms |")
        report_lines.append(f"| å¹³å‡èŠ‚ç‚¹æ•° | {stats['avg_nodes']:.1f} |")
        report_lines.append(f"| å¹³å‡è¾¹æ•° | {stats['avg_edges']:.1f} |")
        report_lines.append(f"| ç¯è·¯æŸ¥è¯¢æ•° | {stats['cycle_count']} |")
        report_lines.append("")
        
        # æŸ¥è¯¢è¯¦æƒ…
        report_lines.append("## æŸ¥è¯¢è¯¦æƒ…")
        report_lines.append("")
        report_lines.append("| æŸ¥è¯¢ID | çŠ¶æ€ | è€—æ—¶(ms) | èŠ‚ç‚¹æ•° | è¾¹æ•° | ç¯è·¯ | é”™è¯¯ |")
        report_lines.append("|--------|------|----------|--------|------|------|------|")
        
        for result in stats.get("query_results", []):
            status_emoji = "âœ…" if result["status"] == "completed" else "âŒ" if result["status"] == "failed" else "â°"
            cycle_emoji = "âš ï¸" if result.get("has_cycles") else "-"
            error = result.get("error", "-")[:20] if result.get("error") else "-"
            report_lines.append(
                f"| {result['query_id']} | {status_emoji} | {result['duration_ms']:.0f} | "
                f"{result['node_count']} | {result['edge_count']} | {cycle_emoji} | {error} |"
            )
        
        # ä¿å­˜æŠ¥å‘Š
        report_dir = self.output_dir / category
        report_dir.mkdir(parents=True, exist_ok=True)
        report_path = report_dir / f"{stats['dataset']}_analysis.md"
        
        with open(report_path, "w", encoding="utf-8") as f:
            f.write("\n".join(report_lines))
        
        print(f"  ğŸ“„ åˆ†ææŠ¥å‘Š: {report_path}")
    
    def generate_overall_analysis(self, all_stats: List[Dict[str, Any]]):
        """ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š"""
        report_lines = []
        
        report_lines.append("# çœŸå®Multi-Agentæ‰§è¡Œç»¼åˆåˆ†ææŠ¥å‘Š")
        report_lines.append("")
        report_lines.append(f"> ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report_lines.append(f"> Benchmark ID: real_benchmark_{self.timestamp}")
        report_lines.append("")
        
        total_queries = sum(s['total_queries'] for s in all_stats)
        total_completed = sum(s['completed'] for s in all_stats)
        total_duration = sum(s['total_duration_ms'] for s in all_stats)
        
        report_lines.append("## æ€»ä½“æ¦‚è§ˆ")
        report_lines.append("")
        report_lines.append("| æŒ‡æ ‡ | å€¼ |")
        report_lines.append("|------|-----|")
        report_lines.append(f"| æ•°æ®é›†æ•°é‡ | {len(all_stats)} |")
        report_lines.append(f"| æ€»æŸ¥è¯¢æ•° | {total_queries} |")
        report_lines.append(f"| æˆåŠŸæ•° | {total_completed} |")
        success_rate = total_completed/total_queries*100 if total_queries > 0 else 0
        report_lines.append(f"| **æˆåŠŸç‡** | **{success_rate:.1f}%** |")
        report_lines.append(f"| **æ€»è€—æ—¶** | **{total_duration/1000:.2f}s** |")
        avg_duration = total_duration/total_completed if total_completed > 0 else 0
        report_lines.append(f"| å¹³å‡è€—æ—¶ | {avg_duration:.2f}ms |")
        throughput = total_completed/(total_duration/1000) if total_duration > 0 else 0
        report_lines.append(f"| **ååé‡** | **{throughput:.2f} QPS** |")
        report_lines.append("")
        
        # æ•°æ®é›†å¯¹æ¯”
        report_lines.append("## æ•°æ®é›†æ€§èƒ½å¯¹æ¯”")
        report_lines.append("")
        report_lines.append("| æ•°æ®é›† | ç±»åˆ« | æˆåŠŸ/æ€»æ•° | å¹³å‡è€—æ—¶(ms) | èŠ‚ç‚¹æ•° | è¾¹æ•° |")
        report_lines.append("|--------|------|-----------|--------------|--------|------|")
        
        for stats in sorted(all_stats, key=lambda x: -x['avg_duration_ms']):
            report_lines.append(
                f"| {stats['dataset']} | {stats['category']} | "
                f"{stats['completed']}/{stats['total_queries']} | "
                f"{stats['avg_duration_ms']:.0f} | {stats['avg_nodes']:.1f} | {stats['avg_edges']:.1f} |"
            )
        
        report_lines.append("")
        
        # è°ƒåº¦ä¼˜åŒ–å»ºè®®
        report_lines.append("## è°ƒåº¦ä¼˜åŒ–å»ºè®®")
        report_lines.append("")
        
        if total_completed > 0:
            # æ‰¾å‡ºç“¶é¢ˆ
            slowest = max(all_stats, key=lambda x: x['avg_duration_ms'])
            report_lines.append(f"- **æœ€æ…¢ç±»å‹**: `{slowest['category']}` ç±»ä»»åŠ¡å¹³å‡è€—æ—¶ {slowest['avg_duration_ms']:.0f}ms")
            
            # åˆ†æèŠ‚ç‚¹å¼€é”€
            avg_nodes = sum(s['avg_nodes'] for s in all_stats) / len(all_stats)
            if avg_nodes > 10:
                report_lines.append(f"- **è°ƒåº¦å¼€é”€å¤§**: å¹³å‡ {avg_nodes:.0f} ä¸ªèŠ‚ç‚¹/æŸ¥è¯¢ï¼Œå»ºè®®åˆå¹¶ç›¸ä¼¼æ“ä½œ")
            
            # åˆ†æå¹¶è¡Œæ½œåŠ›
            report_lines.append("- **å¹¶è¡ŒåŒ–å»ºè®®**: å½“å‰ä¸ºä¸²è¡Œæ‰§è¡Œï¼Œå¯æ ¹æ®Workerä¾èµ–å…³ç³»å®ç°å¹¶è¡Œè°ƒåº¦")
        
        # ä¿å­˜æŠ¥å‘Š
        report_path = self.output_dir / "overall_analysis.md"
        with open(report_path, "w", encoding="utf-8") as f:
            f.write("\n".join(report_lines))
        
        # ä¿å­˜JSON
        summary_path = self.output_dir / "benchmark_summary.json"
        with open(summary_path, "w", encoding="utf-8") as f:
            json.dump({
                "timestamp": self.timestamp,
                "mode": "real_execution",
                "total_queries": total_queries,
                "total_completed": total_completed,
                "total_duration_ms": total_duration,
                "throughput_qps": throughput,
                "datasets": all_stats
            }, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ“Š ç»¼åˆåˆ†æ: {report_path}")
        print(f"ğŸ“‹ JSONæ±‡æ€»: {summary_path}")


async def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("ğŸš€ çœŸå®Multi-Agent Benchmarkæµ‹è¯•")
    print("=" * 60)
    print("âš ï¸ æ³¨æ„: æ­¤è„šæœ¬ä¼šè°ƒç”¨çœŸå®LLM APIï¼Œæ¶ˆè€—APIé¢åº¦")
    print("=" * 60)
    
    data_dir = Path(__file__).parent / "data"
    
    if not data_dir.exists():
        print(f"\nâš ï¸ æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_dir}")
        print("è¯·å…ˆè¿è¡Œ: python benchmarks/download_datasets.py")
        return
    
    runner = RealBenchmarkRunner()
    print(f"\nğŸ“ è¾“å‡ºç›®å½•: {runner.output_dir}")
    
    # ä½¿ç”¨ç¤ºä¾‹æ•°æ®é›†è¿›è¡Œæµ‹è¯•ï¼ˆèŠ‚çœAPIæ¶ˆè€—ï¼‰
    test_datasets = [
        ("math", "sample_math"),
        ("history", "sample_history"),
        ("qa", "sample_qa"),
        ("code", "sample_code")
    ]
    
    all_stats = []
    
    for category, dataset_name in test_datasets:
        dataset_path = data_dir / category / f"{dataset_name}.json"
        if dataset_path.exists():
            try:
                stats = await runner.run_dataset(
                    dataset_path=dataset_path,
                    category=category,
                    max_queries=3  # æ¯ä¸ªæ•°æ®é›†åªæµ‹è¯•3ä¸ªquery
                )
                all_stats.append(stats)
            except Exception as e:
                print(f"âŒ æ•°æ®é›† {dataset_name} æ‰§è¡Œå¤±è´¥: {e}")
    
    if all_stats:
        runner.generate_overall_analysis(all_stats)
    
    print("\n" + "=" * 60)
    print("âœ… çœŸå®Benchmarkæµ‹è¯•å®Œæˆ!")
    print("=" * 60)


if __name__ == "__main__":
    asyncio.run(main())
